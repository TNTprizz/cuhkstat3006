\documentclass[a4paper]{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage[colorlinks=true, urlcolor=blue, linkcolor=blue]{hyperref}
\usepackage{amsmath}


\begin{document}

% I can't believe chktex doesn't allow easy half-closed interval.

% chktex-file 9
% chktex-file 17

\title{STAT3006 Notes}
\author{TNTprizz80315}
\date{06/01/2026}
\maketitle

\setlength{\parindent}{0pt}

\section{Foreward}

\textit{
Hello. Despite the fact that you are not interested in me as a human at all, I am still going to introduce myself.  
    I am TNTprizz80315, and I am making this notes because I hate myself.
}

\textit{This piece of note exists as I am trying to figure out what the course is talkng about thanks to Prof.\ W**'s video playing during the lecture.}

\textit{I would try not to use complex wordings or a bunch of Mathematical notations (like $ \Vvdash, \succnapprox, \gnapprox  $) for readability.}

\textit{Anyways, (sarcastic) enjoy this course and (sarcastic) get a good grade by attending the lecture.}

\section{Outline and Syllabus}

\begin{itemize}
    \item Grading Policy:
    \begin{itemize}
        \item 50\% 4 Assignments
        \item 50\% Final Exam (No Midterm)
    \end{itemize}
    \item Textbooks:
    \begin{itemize}
        \item Computational Statistics, Geof H. Givens and Jennifer A. Hoeting
        \item Numerical Analysis for Statisticians, Kenneth Lange
        \item Monte Carlo Statistical Methods, Christian Robert and
        
        George Casella
    \end{itemize}
    \item Tutorial:
    \begin{itemize}
        \item TBA
    \end{itemize}
\end{itemize}

\textit{Before everything start, I'd like to give you a suggestion: Drop the course if possible. 
I'm here simply because I can't graduate if I don't do so. You know what I mean if you attend a lecture.}

\break{}

\section{Introduction}

\subsection{Why are we learning statistics?}

\textit{(Really? After studying STAT2001 STAT3008 STAT2006???)}

\begin{itemize}
    \item Get accurate results
    \item efficiently (with a relatively small samples)
    \item and come up with conclusions for prediction.
\end{itemize}

Samples $\rightarrow$ Statistical Interpretation $\rightarrow$ conclusions

\subsection{Example Declaration}

We are using this later on, so just try to memorize this.  

\indent

Let's say we are trying to analyze the height of people in a certain region. A typical solution is to collect all people's height, and process the data to get the mean or variance etc.  

However, this is not cool at all, as you need massive computation power to process the thing, not to mention the workload of data collecting.

So, we will just take certain samples (conducting surveys on certain people etc.) and hope that it can help us explain something about the height of them.

We assume that $X \stackrel{}{\sim} N(\mu, 1)$

\indent

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{src/DataVsSample.png}
    \caption{The relationship between all data and samples}\label{fig:DVS}
\end{figure}

As we see in Figure~\ref{fig:DVS}, We are doing sampling (survey, etc.) to obtain samples $\overrightarrow{x}: x_1, x_2,\ldots, x_n$ from the target. The samples are Independent and Identically Distributed, and we are going to use stratified computing to get a result.

Also, the samples can tell something about the entire data, like guessing the mean $(\mu)$ and the variance $(\sigma^2)$. We can do this by the \textbf{Method of Moments} and \textbf{Maximum Likelihood Estimator (MLE)}.

\indent

We are focusing on the latter part as sample collecting is introduced in another course.

\break{}

\subsection{Two Schools of Statistics}

\subsubsection{Frequencies}

In this theorey, we assume that the statistics have:
\begin{itemize}
    \item Fixed parameters, and
    \item Random samples/data
\end{itemize}

Referring to the example above, we use MLE to come up with a guess on the parameter of a model.\ i.e.\ our goal is to obtain $\hat{\mu}$ or $\hat{\sigma^2}$.

To do this, we find $\hat{\mu}$ in which $L(\mu| \overrightarrow{x})$ or $l(\mu| \overrightarrow{x})$ is the maximum, such that
\begin{flalign*}
    P(\mu| x) &= \frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{{(\mu - x)}^2}{2\sigma^2}) = \frac{1}{\sqrt{2\pi}}\exp(-\frac{{(\mu - x)}^2}{2}) &\\
    L(\mu| \overrightarrow{x}) &= \prod_{i = 1}^n P(\mu| x_i)  &\\
    l(\mu| \overrightarrow{x}) &= \ln(\prod_{i = 1}^n P(\mu| x_i)) = \sum_{i = 1}^{n} \ln(P(\mu| x_i)) = -\frac{1}{2}\ln(2\pi)-\frac{\sum_{i=1}^{n}{(\mu - x_i)}^2}{2} &\\
\end{flalign*}

\indent
Both of them works as maximizing $L(\mu| \overrightarrow{x})$ implies maximizing $l(\mu| \overrightarrow{x})$ and vice versa.

\indent

To do so, we can do the following:
\begin{flalign*}
    \frac{d l(\mu| \overrightarrow{x})}{d \mu} \vert_{\hat{\mu}} &= 0 &\\
    \frac{d l(\mu| \overrightarrow{x})}{d \mu} \vert_{\hat{\mu}} &= \frac{2\sum_{i=1}^{n}(\hat{\mu} - x_i)}{2} = 0 &\\
    n\hat{\mu} &= n\bar{x} \rightarrow \hat{\mu} = \bar{x} &
\end{flalign*}
Nice and neat, right? However, the \textbf{Optimization problem} arises. $\hat{\mu}$'s accuracy increases when the number of samples increases. Thats why we are learning how to get accurate $\hat{\mu}$ using least samples/highest efficiency.

\subsubsection{Bayesian}

In this theorey, we assume that the statistics
\begin{itemize}
    \item has random parameters and samples/data
    \item can have prior beliefs about parameters (Guess the parameters as well)
\end{itemize}

We introduce $\pi(\mu)$ into the model to estimate $\mu$ as well so that it is updated after observing the data.

\indent

Bayes' theorem:
\begin{equation} \label{eq:bayesian}
    P(A|B) = \frac{P(A, B)}{P(B)} = \frac{P(A)P(B|A)}{P(B)}
\end{equation}

Then, we get the following by pluging in (\ref{eq:bayesian}):
\begin{flalign*}
    P(\mu|\overrightarrow{x}) &= \frac{P(\mu,\overrightarrow{x})}{P(\overrightarrow{x})} = \frac{\pi(\mu)P(\overrightarrow{x}|\mu)}{\int\pi(\tau)P(\overrightarrow{x}|\tau)\,d\tau} = c\cdot\pi(\mu)P(\mu|\overrightarrow{x}) \propto \pi(\mu)P(\overrightarrow{x}|\mu) &
\end{flalign*}

Now, we get two things:

\begin{itemize}
    \item $\overrightarrow{x} \stackrel{i.i.d}{\sim} N(\mu, 1)$ \textit{(Note that $\mu$ is unknown, and is the target of inference.)}
    \item $\pi(\mu) \sim N(a, b^2)$, where $b^2$ is very large
\end{itemize}

Our goal is to get the distribution of $\mu$ instead of $\pi(\mu)$, so:

\indent

Given that if $f(x) = c_0 m(x)$ and $h(x) = d_0 m(x)$ are density functions,
\begin{flalign*}
    c_0 &= d_0, f(x) = h(x) &
\end{flalign*}

Proof: even monkey knows that if $f(x)$ is a density function, $\int_{-\infty}^{+\infty} f(x) \,dx = 1$.

Then, $\int_{-\infty}^{+\infty} c_0 m(x) \,dx = \int_{-\infty}^{+\infty} d_0 m(x) \,dx = 1 \rightarrow c_0 = d_0 \rightarrow f(x) = h(x) \rightarrow \square$

\begin{flalign*}
    P(\mu|\overrightarrow{x}) \propto \pi(\mu)P(\overrightarrow{x}|\mu) &\propto \frac{1}{\sqrt{2\pi}b}\exp(-\frac{{(\mu - a)}^2}{2b^2}) \cdot \prod_{i = 1}^{n}\frac{1}{\sqrt{2\pi}}\exp(-\frac{{(x_i - \mu)}^2}{2}) &\\
    &\propto \exp(-\frac{{(\mu - a)}^2}{2b^2} - \frac{\sum_{i = 1}^{n}{(x_i - \mu)}^2}{2}) &\\
    &\propto \exp(-\frac{1}{2}(\frac{\mu^2 - 2\mu a + a^2}{b^2} - \sum_{i=1}^{n}x_i^2 - 2n\mu \bar{x} + n\mu^2)) &\\
    &\propto \exp(-\frac{1}{2}(\mu^2(\frac{1}{b^2} + n) - 2\mu(\frac{a}{b^2} + n\bar{x}))) &\\
    &\propto \exp(-\frac{{(\mu - \frac{\frac{a}{b^2} + n \bar{x}}{\frac{1}{b^2} + n})}^2}{2\cdot\frac{1}{\frac{1}{b^2}+n}})
\end{flalign*}


We surprisingly found that
\begin{equation}
\mu|\overrightarrow{x} \sim N(\frac{\frac{a}{b^2} + n \bar{x}}{\frac{1}{b^2} + n}, \frac{1}{\frac{1}{b^2}+n})
\end{equation}

Then, the posterior mean $E(\mu|\overrightarrow{x})$
\begin{flalign*}
    &= \frac{\frac{a}{b^2} + n \bar{x}}{\frac{1}{b^2} + n} = \frac{\frac{a}{n\cdot b^2} +\bar{x}}{\frac{1}{n\cdot b^2} + 1} \stackrel{n \rightarrow +\infty}{\longrightarrow}\bar{x} &
\end{flalign*}

the posterior variance $Var(\mu|\overrightarrow{x})$
\begin{flalign*}
    &= \frac{1}{\frac{1}{b^2}+n} \lessapprox \frac{1}{n} = Var(\bar{x}) = \frac{Var(x_i)}{n} = \frac{1}{n} &
\end{flalign*}
\indent

In general, direct maximization of $P(\mu|\overrightarrow{x})$ is difficult, so we approximate 

$P(\mu|\overrightarrow{x})$ by sampling, which is something like $\overrightarrow{\mu}: \mu_1. \mu_2, \ldots, \mu_m$.

The probelm now becomes: How do we sample from a target distribution? (i.e. What the hell is $\overrightarrow{\mu}$???)

\subsection{Grouped Data Analysis}

There might exist different distributions for different groups of population. For instance, male's average height is \textit{generally} higher than female's.

In this case, we divide the population into two groups: Female and male.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{src/HightDistribn.png}
    \caption{Visible relationship between the height of female and male}\label{fig:HD}
\end{figure}

What we are finding now is $P(x | \mu_1, \mu_2, \sigma_1^2, \sigma_2^2, p)$, where $p$ stands for the probability of data $x_i$ is from a female.

\indent

Obviously:
\begin{flalign*}
    P(x | \mu_1, \mu_2, \sigma_1^2, \sigma_2^2, p) &= p \cdot P(x| p = 1) + (1-p) \cdot P(x|p=0) &\\
    &=p \cdot \frac{1}{\sqrt{2\pi}\sigma_1}\exp(-\frac{{(x - \mu_1)}^2}{2\sigma_1^2}) &\\
    &+ (1-p) \cdot \frac{1}{\sqrt{2\pi}\sigma_2}\exp(-\frac{{(x - \mu_2)}^2}{2\sigma_2^2}) &\\
\end{flalign*}

\break{}

Consider $l(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, p | \overrightarrow{x})$ (We don't do $L(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, p | \overrightarrow{x})$ as it is too difficult to do.): 
\begin{flalign*}
    &\sum_{i = 1}^{n} \ln(p \cdot \frac{1}{\sqrt{2\pi}\sigma_1}\exp(-\frac{{(x - \mu_1)}^2}{2\sigma_1^2}) + (1-p) \cdot \frac{1}{\sqrt{2\pi}\sigma_2}\exp(-\frac{{(x - \mu_2)}^2}{2\sigma_2^2})) &
\end{flalign*}
Then we find:
\begin{flalign*}
    &\frac{dl}{d \mu_1} \vert_{\hat{\mu}} = 0, \frac{dl}{d \mu_2} \vert_{\hat{\mu}} = 0, \frac{dl}{d \sigma_1^2} \vert_{\hat{\mu}} = 0, \frac{dl}{d \sigma_2^2} \vert_{\hat{\mu}} = 0, \frac{dl}{dp} \vert_{\hat{\mu}} = 0&
\end{flalign*}
\textit{Don't do this, as you can see how complex it would be when you actually do the differentiation.}

\indent

To deal with this, we introduce $\overrightarrow{z}: z_1, z_2, \ldots , z_n$ where

\[
\left\{ 
\begin{array}{lr}
z_i = 1&(female) \\
z_i = 0&(male)
\end{array}
\right.
\]

Then, $P(x_i, z_i | \mu_1, \mu_2, \sigma_1^2, \sigma_2^2, p) =$
\begin{flalign*}
    & {(\frac{p}{\sqrt{2\pi}\sigma_1} \exp(-\frac{{(x_i - \mu_1)}^2}{2\sigma_1^2}))}^{z_i} \cdot {(\frac{1-p}{\sqrt{2\pi}\sigma_2}\exp(-\frac{{(x_i - \mu_2)}^2}{2\sigma_2^2}))}^{1 - z_i} &
\end{flalign*}
Now, we can get a proper $l(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, p | \overrightarrow{x}, \overrightarrow{z})$ By
\begin{flalign*}
    \ln(&\prod^{n}_{i = 1}({(\frac{p}{\sqrt{2\pi}\sigma_1} \exp(-\frac{{(x_i - \mu_1)}^2}{2\sigma_1^2}))}^{z_i} \cdot {(\frac{1-p}{\sqrt{2\pi}\sigma_2}\exp(-\frac{{(x_i - \mu_2)}^2}{2\sigma_2^2}))}^{1 - z_i})) &\\
    &= \sum_{i = 1}^{n}( z_i\cdot(\ln(p) - \frac{1}{2}\ln(2\pi\sigma_1^2) - \frac{{(x_i - \mu_1)}^2}{2\sigma_1^2}) &\\
    &+(1-z_i)\cdot(\ln(1 - p) - \frac{1}{2}\ln(2\pi\sigma_2^2) - \frac{{(x_i - \mu_2)}^2}{2\sigma_2^2}) ) 
\end{flalign*}
Now, we get $\hat{p}$ and $\hat{\mu}$ by:
\begin{flalign*}
    \frac{dl}{dp} \vert_{\hat{\mu}} &= \sum_{i = 1}^{n} (\frac{z_i}{p} - \frac{1-z_i}{1-p}) = 0 \Rightarrow \hat{p} = \bar{z} &\\
    \frac{dl}{d\mu_1} \vert_{\hat{\mu}} &= \sum_{i=1}^{n}(\frac{z_i \cdot 2 \cdot (x_i - \hat{\mu})}{2\sigma_1^2}) = 0 \Rightarrow \hat{\mu} = \frac{\sum_{i = 1}^{n}z_i \cdot x_i}{n\cdot\bar{z}}
\end{flalign*}
We can obviously see that $\hat{p}$ is the proportion of females in the samples, while $\hat{\mu}$ is the average height of female.

\indent

This will only work if we know what $\overrightarrow{z}$ is, but we know nothing about this!

In this case, we guess what $\overrightarrow{z}$ is from $\overrightarrow{x}$.
\textit{We are going to talk about this later in this course (maybe).}

\break{}

\section{Solution to Non-linear Equations}

\subsection{Bisection Method}

We declare a function $g(x)$ as an example, and we are trying to find $x$ when $g(x) = 0$.

$g(x)$ has the following properties:

\begin{itemize}
    \item $g(x) \in C[a, b]$ ($g(x)$ is continuous between $a$ and $b$)
    \item $g(a) \cdot g(b) < 0$ ($g(a)$ and $g(b)$ are opposite in sign (one +ve, one -ve))
\end{itemize}

\paragraph{Intermediate Value Theorem}

\indent

Consider a continuous function $f: [a, b] \in \mathbb{R} \rightarrow \mathbb{R}$, if there exists a $u$ such that $f(a) < u < f(b)$ or $f(a) > u > f(b)$, then there must exist a $c \in [a, b]$ such that $f(c) = u$.

\indent

To find $c$ such that $g(c) = 0$ such that $c \in [a, b]$, we may cut the interval in half and find which part $c$ is in. We repeat this process until $c$'s accuracy is high enough.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{src/IVTDemo.png}
    \caption{An example of how to find $x$ when $g(x) = 0$}\label{fig:IVT}
\end{figure}

If $g(a) \cdot g(b) < 0$, $c \in [a, b]$. We can make use of this property to let our computer do the job for us.

\break{}

The program: Refer to \href{https://github.com/tntprizz/cuhkstat3006/tree/main/example_programs/ivt.rs}{here} for an executable program.

\begin{verbatim}
fn g(x: f32) -> f32 {3.0 * x.powf(2.0) - 5.0}

fn ivt(maxit: u32, mut a: f32, mut b: f32, ts: f32) -> f32 {
    let mut c: f32 = (a + b) / 2.0;
    for _ in 1..=maxit {
        if g(c) == 0.0 || (((b - a) / 2.0)) < ts {break;}
        if g(c)*g(a) > 0.0 {a=c;} else {b=c}
        c = (a + b) / 2.0;
    }
    c
}
\end{verbatim}

\paragraph{Example: Finding the shortest C.I.}

\indent

Let's say we have a function, and we want the shortest interval $[a, b]$ so that $\int_{b}^{a} f(x) \,dx = \alpha_0 = 0.95$ while $f(b) = f(a) = \lambda_{0.95}$.

Seems complicated, but in a nutshell:

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{src/CIEx.png}
    \caption{The nutshell}\label{fig:CIEx}
\end{figure}

We divide the problem into two parts:

\paragraph{1. How do we find $a$ and $b$ given $\lambda$?}
\indent
Bisection method. Stated above.

\paragraph{2. How do we find the most suited $\lambda_{0.95}$?}
\indent
Trial-and-error.

If the initial $\lambda_{0.95}$ doesn't suit, try to declare $\lambda_{lower}$ or $\lambda_{higher}$ which covers the correct $\lambda_{0.95}$. Apply the bisection method to get the number.

\indent

Now, we can get the required interval using computers efficiently.

Refer to \href{https://github.com/tntprizz/cuhkstat3006/tree/main/example_programs/FindCI.rs}{here} for an example program.

\break{}

\subsection{Functional Iteration}

Let me remind you what we are trying to do here: $g(x_0) = 0$, what is $x_0$?

\indent

To do this, we are going to declare $f(x) = g(x) + x$ so that

\[f(x_0) = g(x_0) + x_0 = 0 + x_0 = x_0\]


What we've done here is basically turning zero point of $g(x)$ to a fixed point of $f(x)$.

In order to increase the accuracy, we are going to recurse $f(x)$, as we can see:

\[x^{(0)} \rightarrow x^{(1)} = f(x^{(0)}) \rightarrow x^{(2)} = f(x^{(1)}) = f(f(x^{(0)}))\]

and so on until the accuracy is satisfactory.\ (maybe get $x^{(20)}$ and treat it as $x_0$)

\hypertarget{anc: FIPrep}{\paragraph{Preposition}} \indent Suppose $f: I \rightarrow \mathbb{R}$
\begin{itemize}
    \item $f(x) \in I, \forall x \in I$ (Map itself to itself, so that both of them lies inside $I$)
    \item $|f(y) - f(x)| \leq \lambda |y - x|, \forall x, y \in I$ (Lipschitz condition)
\end{itemize}

Then, if $\lambda \in [0, 1)$, we have $|f(y) - f(x)| < |y - x|$ (Meaning that the distance between $x$ and $y$ decreases.) This implies that:

\begin{itemize}
    \item $f(x)$ has a unique fixed point $x_{\infty} \in I$.
    \item $x_n = f(x_{n - 1}) \rightarrow x_{\infty}, \forall x_0 \in I$
    \item $|x_n - x_\infty| \leq \frac{\lambda^n}{1 - \lambda} \cdot |x_1 - x_\infty|$
\end{itemize}

Proof:
\begin{flalign*}
    |x_{k+1} - x_k| = |f(x_k) - f(x_{k-1})| &\leq \lambda \cdot |x_k - x_{k-1}| & \\
    &= \lambda \cdot | f(x_{k-1}) - f(x_{k-2}) | \\
    &\leq \lambda^2 \cdot |x_{k-1} - x_{k-2}| \\
    &\leq \lambda^k \cdot |x_1 - x_0|
\end{flalign*}
\begin{flalign*}
    |x_m - x_n| &= |x_m - x_{m-1} + x_{m-1}-x_{m-2}+x_{m-2}+ \cdots +x_{n-1} + x_n| &\\
    & \leq \sum_{k = n}^{m - 1} |x_{k+1} - x_k| &\\
    & \leq \sum_{k - n}^{m - 1} \lambda^k \cdot |x_1 - x_0| &\\
    & = |x_1 - x_0|  \cdot \sum_{k = n}^{m - 1} \lambda^k &\\
    & \leq |x_1 - x_0| \cdot \sum_{k = n}^{\infty} \lambda^k &\\
    & = |x_1 - x_0| \cdot \frac{\lambda^n}{1 - \lambda} & \rightarrow \square
\end{flalign*}

\break{}

\paragraph{Cauchy Sequence} \indent $\forall \epsilon > 0, \exists N_\epsilon$ such that $\forall m, n \geq N_\epsilon, N \in \mathbb{N}$:
\begin{flalign*}
    &|x_m - x_n| < \epsilon &
\end{flalign*}
\begin{flalign*}
    |x_m-x_n| &< \epsilon &\\
    |x_1 - x_0| \cdot \frac{\lambda^{N_\epsilon}}{1 - \lambda} &< \epsilon &\\
    \lambda^{N_\epsilon} &< \frac{\epsilon(1-\lambda)}{|x_1 - x_0|} &\\
    N_\epsilon &> \log_\lambda(\frac{\epsilon(1-\lambda)}{|x_1 - x_0|}) &
\end{flalign*}

$\implies \{x_n\} _{n = 1}^\infty$ is a Cauchy Sequence. \textit{We don't need to know what it is.}

\paragraph{Lipschitz Condition} \indent When $f$ is a continuous function,
\begin{flalign*}
    x_\infty = \lim_{n \rightarrow \infty} x_n = \lim_{n \rightarrow \infty} f(x_{n-1}) &= \lim_{n \rightarrow \infty} f(x_n) &\\
    &= f(\lim_{n \rightarrow \infty} x_n) &\\
    &= f(x_\infty) &
\end{flalign*}

$\implies x_\infty$ is a fixed point of $f(x)$.

\indent

If there exists $y_\infty \neq x_\infty$ such that $f(y_\infty) = y_\infty$,
\begin{flalign*}
    |y_\infty - x_\infty| &= |f(y_\infty) - f(x_\infty)| &\\
    & \leq \lambda \cdot |y_\infty - x_\infty| &\\
    & < |y_\infty - x_\infty| & \rightarrow \bot   
\end{flalign*}

$\implies I$ is a closed interval, $x_\infty \in I$.

\paragraph{Example: find $\sqrt{a}$}

\indent

Solution 1: $\sqrt{a}$ is a solution to the equation $\frac{1}{2} \cdot (\frac{a}{x} - x) = 0$, so we declare
\begin{align*}
    g(x) &= \frac{1}{2} \cdot (\frac{a}{x} - x) & f(x) &= g(x) + x = \frac{1}{2} \cdot (\frac{a}{x} - x) + x = \frac{1}{2} \cdot (\frac{a}{x} + x) &
\end{align*}

Then, we iterate by letting the resultant $f(x)$ become the next $x$.

\break{}

The program: Refer to \href{https://github.com/tntprizz/cuhkstat3006/tree/main/example_programs/FIter.rs}{here} for an executable program.

\begin{verbatim}
fn g(a: f64, x: &f64) -> f64 {
    0.5 * ((a / *x) - *x)
}
fn iterate(maxit: u32, a: f64, x: &mut f64, it: &mut u32)
-> f64 {
    if *it >= maxit {return *x;}
    *it += 1;
    *x = g(a, x) + *x;
    iterate(maxit, a, x, it)
}
fn main() {
    let mut x: f64 = 1.4;
    let mut it: u32 = 0;

    println!("{}", iterate(4, 2.0, &mut x, &mut it));
}
\end{verbatim}

\indent

Solution 2 \textbf{(Wrong)}: $\sqrt{a}$ is a solution to the equation $\frac{a}{x} - x$, so we declare
\begin{align}
    g(x) &= \frac{a}{x} - x & f(x) &= g(x) + x = \frac{a}{x} &
\end{align}
The modified program will refuse to run correctly. Why?

\indent

We Get

\begin{flalign*}
    |f(y) - f(x)| &= |\frac{a}{y} - \frac{a}{x}| = \frac{a}{x \cdot y} \cdot |y - x| &
\end{flalign*}
Now, we need to find $I = [c, d]$ such that

\begin{itemize}
    \item $f(x) \in [c, d]$, $\forall x \in [c, d] \implies \sqrt{a} \in [c, d]$
    \item $\frac{a}{x \cdot y} \leq \lambda < 1, \exists x, y \in [c, d]$
    \begin{flalign*}
        &\rightarrow x \leq c, y \leq c \implies xy > c^2 \implies \frac{a}{xy} < \frac{a}{c^2} &
    \end{flalign*}
\end{itemize}
Let $x, y$ be something very close to $c$:
\begin{flalign*}
    &\rightarrow \frac{a}{c^2} < 1 \implies c^2 > a \rightarrow c > \sqrt{a} \implies \sqrt{a} \notin [c, d] &\rightarrow \bot
\end{flalign*}

\break{}

Now, when $f(x) = \frac{1}{2} \cdot (\frac{a}{x} + x)$:
\begin{flalign*}
    |f(y) - f(x)| &= \frac{1}{2} \cdot |(\frac{a}{y} - \frac{a}{x}) + (y - x)| &\\
    &= \frac{1}{2} \cdot |\frac{a}{x \cdot y} \cdot (x - y) - (x - y)| &\\
    &= \frac{1}{2} \cdot |\frac{a}{x \cdot y} - 1| \cdot |y - x| &
\end{flalign*}
\begin{flalign*}
    &I: [\sqrt{\frac{2a}{3}}, \sqrt{2a}], \sqrt{a} \in I&
\end{flalign*}
\begin{align*}
    &\forall x, y \in I &\implies \sqrt{\frac{2a}{3}} \leq x \leq \sqrt{2a} & &\sqrt{\frac{2a}{3}} \leq y \leq \sqrt{2a} &
\end{align*}
\begin{flalign*}
    \frac{1}{2} &\leq &&\frac{a}{x \cdot y} &\leq \frac{3}{2} &\\
    -\frac{1}{2} &\leq &&\frac{a}{x \cdot y} - 1 &\leq \frac{1}{2} &\\
    -\frac{1}{2} \cdot \frac{1}{2} &\leq &&\frac{1}{2} \cdot (\frac{a}{x \cdot y} - 1) &\leq \frac{1}{2} \cdot \frac{1}{2} &\\
    & &|&\frac{1}{2} \cdot (\frac{a}{x \cdot y} - 1)| \leq \frac{1}{4} < 1 && \rightarrow \square
\end{flalign*}

\break{}

Now, how do we verify whether $f(x)$ satisfies the \hyperlink{anc: FIPrep}{Prepositions?}

Whenever we can find a neibourhood around $x_\infty$ such that $|f'(x)|$ within the neibourhood is bounded by $0 < \lambda \leq 1$.

\paragraph{Lagrange's Mean Value Theorem}

\indent

If $f$ is continuous on the closed interval $[a, b]$ and is differentiable on the open interval $(a, b)$, then there exists a point $\xi \in (a, b)$ such that
\begin{flalign*}
    f'(\xi) &= \frac{f(b) - f(a)}{b - a} &
\end{flalign*}

\textit{The slope when $x = \xi$ is the average slope between $a$ and $b$.}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.75\linewidth]{src/LMVTDemo.png}
    \caption{What LMVT is talking about}\label{fig:LMVTDemo}
\end{figure}

\begin{flalign*}
    f(b) - f(a) =& f'(\xi) \cdot (b - a) &\\
    f(b) =& f(a) + f'(\xi) \cdot (b - a) &\\
    \implies |f(y) - f(x)| \leq& \lambda \cdot |y - x| &\\
    =& |f(x) + f'(\xi_{xy}) \cdot (y - x) - f(x) | &\\ 
    =& | f'(\xi_{xy}) | \cdot | y - x |
\end{flalign*}
If we can find an interval $I = [c, d]$ such that $x_\infty \in I$ and $\sup |f'(\xi)| \leq \lambda < 1$, $\xi \in [c, d]$. (We want to find $x_\infty$)

Now, we let $I = [c, d] = [x_\infty - \alpha, x_\infty + \alpha]$ so that
\begin{align*}
    -\alpha &\leq &&f(x) - x_\infty &\leq& \alpha \\
    x_\infty - \alpha &\leq &&f(x) &\leq& x_\infty + \alpha \\
\end{align*}
However, where does the error $\alpha$ comes from??? $\forall x \in I$
\begin{flalign*}
    |x - x_\infty| &\leq \alpha &\\
    |f(x) - x_\infty| =& |f(x) - f(x_\infty)| &\\
    \stackrel{LMVT}{=} &|f'(\xi)| \cdot |x - x_\infty| &\\
    \leq& \sup |f'(\tau)| \cdot |x - x_\infty| &(\tau \in [c, d])\\
    \leq& \lambda \cdot |x - x_\infty| < |x - x_\alpha| \leq \alpha &
\end{flalign*}

\break{}

As a result, we found that it
\begin{itemize}
    \item Map itself by itself.
    \item Their distance decreased after mapping.
\end{itemize}

\subsection{Newton's Method}



\end{document}